{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruthfulQA\n",
    "\n",
    "TruthfulQA differs quite a bit from the other tasks as there is not a single correct answer. Interestingly, all the true responses come before the false responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeno_client import ZenoClient, ZenoMetric\n",
    "import datasets\n",
    "import os\n",
    "\n",
    "API_KEY = os.environ[\"ZENO_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the list of models used.\n",
    "You can go to the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) to check what models are available.\n",
    "Some of them might not have associated data, you can check this by clicking on the little icon next to the model name.\n",
    "If you get a 404 after clicking, we won't be able to fetch the model data and this notebook will crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"meta-llama/Llama-2-70b-hf\", \"mistralai/Mistral-7B-v0.1\", \"tiiuae/falcon-40b\", \"Riiid/sheep-duck-llama-2-70b-v1.1\", \"AIDC-ai-business/Marcoroni-70B-v1\", \"ICBU-NPU/FashionGPT-70B-V1.1\", \"adonlee/LLaMA_2_70B_LoRA\", \"uni-tianyan/Uni-TianYan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(model: str):\n",
    "    data_path = \"details_\" + model.replace(\"/\", \"__\")\n",
    "    return datasets.load_dataset(\n",
    "        \"open-llm-leaderboard/\" + data_path,\n",
    "        \"harness_truthfulqa_mc_0\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(df):\n",
    "    df_lim = df[[\"question\", \"mc2_targets\"]]\n",
    "    df_lim.loc[:, \"data\"] = df_lim.apply(lambda x: \"\\n\" + x[\"question\"] + \"\\n\\n\" + \"\\n\".join(map(lambda y: \"- \" + str(y).lstrip(), x[\"mc2_targets\"][\"choices\"])), axis=1)\n",
    "    df_lim.loc[:, \"# of options\"] = df_lim.apply(lambda x: len(x[\"mc2_targets\"][\"choices\"]), axis=1)\n",
    "    df_lim.loc[:, \"# of answers\"] = df_lim.apply(lambda x: x[\"mc2_targets\"][\"labels\"].sum(), axis=1)\n",
    "    df_lim.loc[:, \"label\"] = df_lim[\"mc2_targets\"].apply(lambda r: \"\\n\" + \"\\n\".join([\"True\" if s == 1 else \"False\" for s in r[\"labels\"]]))\n",
    "    df_lim = df_lim.drop(columns=[\"mc2_targets\", \"question\"])\n",
    "    df_lim[\"id\"] = df_lim.index \n",
    "    return df_lim\n",
    "\n",
    "def generate_system(df):\n",
    "    df_system = df[[\"predictions\", \"mc2\", \"mc1_targets\"]]\n",
    "    df_system[\"predictions\"] = df_system.apply(lambda x: f\"mc2: {x['mc2']}\" + \"\\n\\n\" + \"\\n\".join(map(lambda y: str(round(y, 2)), x['predictions'][len(x['mc1_targets']['choices']):])), axis=1)\n",
    "    df_system[\"id\"] = df_system.index\n",
    "    return df_system[[\"predictions\", \"mc2\", \"id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have your Zeno API key in your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ZenoClient(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a project to hold the data for the TruthfulQA task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = client.create_project(\n",
    "    name=\"TruthfulQA\", \n",
    "    view=\"text-classification\", \n",
    "    description=\"TruthfulQA (https://arxiv.org/abs/2109.07958) task in the Open-LLM-Leaderboard.\",\n",
    "    metrics=[\n",
    "        ZenoMetric(name=\"accuracy\", type=\"mean\", columns=[\"mc2\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now upload the data to the project we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_dataset(get_data(models[0])['latest'].to_pandas())\n",
    "print(\"\\nYour dataset has {} rows\\n\".format(len(df)))\n",
    "num_rows = len(df)\n",
    "proj.upload_dataset(df, id_column=\"id\", label_column=\"label\", data_column=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us upload all the model outputs for the models we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    dataset = get_data(model)['latest'].to_pandas()\n",
    "    print(dataset.columns)\n",
    "    if len(dataset) != num_rows:\n",
    "        print(\"Skipping {} because it has {} rows instead of {}\".format(model, len(dataset), num_rows))\n",
    "        continue\n",
    "    df_system = generate_system(dataset)\n",
    "    proj.upload_system(df_system[[\"predictions\", \"mc2\", \"id\"]], name=model.replace('/', \"__\"), output_column=\"predictions\", id_column=\"id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
