{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU\n",
    "\n",
    "MMLU is a question answering task where each question has four potential answers, one of which is correct. Questions come from 57 categories, including elementary mathematics, US history, computer science, law, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeno_client import ZenoClient, ZenoMetric\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "API_KEY = os.environ[\"ZENO_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the list of models used.\n",
    "You can go to the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) to check what models are available.\n",
    "Some of them might not have associated data, you can check this by clicking on the little icon next to the model name.\n",
    "If you get a 404 after clicking, we won't be able to fetch the model data and this notebook will crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"meta-llama/Llama-2-70b-hf\", \"mistralai/Mistral-7B-v0.1\", \"tiiuae/falcon-40b\", \"Riiid/sheep-duck-llama-2-70b-v1.1\", \"AIDC-ai-business/Marcoroni-70B-v1\", \"ICBU-NPU/FashionGPT-70B-V1.1\", \"adonlee/LLaMA_2_70B_LoRA\", \"uni-tianyan/Uni-TianYan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 57 tasks in the MMLU dataset\n",
    "tasks = [\"hendrycksTest-abstract_algebra\", \"hendrycksTest-anatomy\", \"hendrycksTest-astronomy\", \"hendrycksTest-business_ethics\", \"hendrycksTest-clinical_knowledge\", \"hendrycksTest-college_biology\", \"hendrycksTest-college_chemistry\", \"hendrycksTest-college_computer_science\", \"hendrycksTest-college_mathematics\", \"hendrycksTest-college_medicine\", \"hendrycksTest-college_physics\", \"hendrycksTest-computer_security\", \"hendrycksTest-conceptual_physics\", \"hendrycksTest-econometrics\", \"hendrycksTest-electrical_engineering\", \"hendrycksTest-elementary_mathematics\", \"hendrycksTest-formal_logic\", \"hendrycksTest-global_facts\", \"hendrycksTest-high_school_biology\", \"hendrycksTest-high_school_chemistry\", \"hendrycksTest-high_school_computer_science\", \"hendrycksTest-high_school_european_history\", \"hendrycksTest-high_school_geography\", \"hendrycksTest-high_school_government_and_politics\", \"hendrycksTest-high_school_macroeconomics\", \"hendrycksTest-high_school_mathematics\", \"hendrycksTest-high_school_microeconomics\", \"hendrycksTest-high_school_physics\", \"hendrycksTest-high_school_psychology\", \"hendrycksTest-high_school_statistics\", \"hendrycksTest-high_school_us_history\", \"hendrycksTest-high_school_world_history\", \"hendrycksTest-human_aging\", \"hendrycksTest-human_sexuality\", \"hendrycksTest-international_law\", \"hendrycksTest-jurisprudence\", \"hendrycksTest-logical_fallacies\", \"hendrycksTest-machine_learning\", \"hendrycksTest-management\", \"hendrycksTest-marketing\", \"hendrycksTest-medical_genetics\", \"hendrycksTest-miscellaneous\", \"hendrycksTest-moral_disputes\", \"hendrycksTest-moral_scenarios\", \"hendrycksTest-nutrition\", \"hendrycksTest-philosophy\", \"hendrycksTest-prehistory\", \"hendrycksTest-professional_accounting\", \"hendrycksTest-professional_law\", \"hendrycksTest-professional_medicine\", \"hendrycksTest-professional_psychology\", \"hendrycksTest-public_relations\", \"hendrycksTest-security_studies\", \"hendrycksTest-sociology\", \"hendrycksTest-us_foreign_policy\", \"hendrycksTest-virology\", \"hendrycksTest-world_religions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_task(model: str, task: str):\n",
    "    data_path = \"details_\" + model.replace(\"/\", \"__\")\n",
    "    return datasets.load_dataset(\n",
    "        \"open-llm-leaderboard/\" + data_path,\n",
    "        f\"harness_{task.replace('-', '_')}_5\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(model: str):\n",
    "    frames = []\n",
    "    for task in tasks:\n",
    "        data = get_data_for_task(model, task)['latest'].to_pandas()\n",
    "        data['task'] = task\n",
    "        frames.append(data)\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def generate_dataset(df):\n",
    "    df_lim = df[[\"example\", \"choices\", \"gold\", \"task\"]]\n",
    "    df_lim.loc[:, \"data\"] = df_lim.apply(lambda x: x[\"example\"][:x[\"example\"].rfind('\\n')], axis=1)\n",
    "    df_lim.loc[:, \"label\"] = df_lim.apply(lambda x: labels[x[\"gold\"]], axis=1)\n",
    "    df_lim = df_lim.drop(columns=[\"example\", \"choices\", \"gold\"])\n",
    "    df_lim[\"id\"] = df_lim.index \n",
    "    return df_lim\n",
    "\n",
    "def generate_system(df):\n",
    "    df_system = df[[\"predictions\", \"acc\", \"choices\"]]\n",
    "    df_system[\"predictions\"] = df_system.apply(lambda x: labels[np.argmax(x['predictions'])] + \"\\n\\n\" + \"Pred.: \" + \", \".join(map(lambda y: str(round(y, 2)), x['predictions'])), axis=1)\n",
    "    df_system[\"correct\"] = df_system.apply(lambda x: True if x['acc'] > 0 else False, axis=1)\n",
    "    df_system = df_system.drop(columns=[\"acc\", \"choices\"])\n",
    "    df_system[\"id\"] = df_system.index\n",
    "    return df_system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have your Zeno API key in your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ZenoClient(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a project to hold the data for the MMLU task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = client.create_project(\n",
    "    name=\"MMLU\", \n",
    "    view=\"text-classification\", \n",
    "    description=\"MMLU (https://arxiv.org/abs/2009.03300) tasks in the Open-LLM-Leaderboard.\",\n",
    "    metrics=[\n",
    "        ZenoMetric(name=\"accuracy\", type=\"mean\", columns=[\"correct\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now upload the data to the project we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_dataset(get_data(models[0]))\n",
    "print(\"\\nYour dataset has {} rows\\n\".format(len(df)))\n",
    "num_rows = len(df)\n",
    "proj.upload_dataset(df, id_column=\"id\", label_column=\"label\", data_column=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us upload all the model outputs for the models we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    dataset = get_data(model)\n",
    "    if len(dataset) != num_rows:\n",
    "        print(\"Skipping {} because it has {} rows instead of {}\".format(model, len(dataset), num_rows))\n",
    "        continue\n",
    "    df_system = generate_system(dataset)\n",
    "    proj.upload_system(df_system, name=model.replace('/', \"__\"), output_column=\"predictions\", id_column=\"id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
