{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Import libraries, set up model, and read input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "import os\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import zeno_client\n",
    "import dotenv\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import wave\n",
    "import struct\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "dotenv.load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"speech_accent_archive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"data\"] = \"https://zenoml.s3.amazonaws.com/accents/\" + df[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get amplitude and length\n",
    "def get_amplitude_and_length_from_url(url):\n",
    "    # Download the WAV file content from the URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "\n",
    "        # Use the BytesIO object as input for the wave module\n",
    "        with wave.open(BytesIO(response.content), 'rb') as wav_file:\n",
    "            frame_rate = wav_file.getframerate()\n",
    "            n_frames = wav_file.getnframes()\n",
    "            n_channels = wav_file.getnchannels()\n",
    "            sample_width = wav_file.getsampwidth()\n",
    "            duration = n_frames / frame_rate\n",
    "\n",
    "            frames = wav_file.readframes(n_frames)\n",
    "            if sample_width == 1:  # 8-bit audio\n",
    "                fmt = '{}B'.format(n_frames * n_channels)\n",
    "            elif sample_width == 2:  # 16-bit audio\n",
    "                fmt = '{}h'.format(n_frames * n_channels)\n",
    "            else:\n",
    "                raise ValueError(\"Only supports up to 16-bit audio.\")\n",
    "            \n",
    "            frame_amplitudes = struct.unpack(fmt, frames)\n",
    "            max_amplitude = max(frame_amplitudes)\n",
    "            max_amplitude_normalized = max_amplitude / float(int((2 ** (8 * sample_width)) / 2))\n",
    "\n",
    "            return max_amplitude_normalized, duration\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Define a wrapper function for apply to work row-wise\n",
    "def apply_get_amplitude_and_length(row):\n",
    "    url = row['data']  # Assuming the URL is in the 'data' column\n",
    "    amplitude, length = get_amplitude_and_length_from_url(url)\n",
    "    return pd.Series({'amplitude': amplitude, 'length': length})\n",
    "\n",
    "# Usage with apply on the DataFrame\n",
    "# This will create two new columns 'amplitude' and 'length' in the DataFrame\n",
    "df[['amplitude', 'length']] = df.progress_apply(apply_get_amplitude_and_length, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeno Project\n",
    "\n",
    "We create a Zeno project with a WER metric and upload our base data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = zeno_client.ZenoClient(os.environ.get(\"ZENO_API_KEY\"))\n",
    "\n",
    "project = client.create_project(\n",
    "    name=\"Transcription Whisper Distil\", \n",
    "    view=\"audio-transcription\",\n",
    "    description=\"Test of audio transcription\",\n",
    "    metrics=[\n",
    "        zeno_client.ZenoMetric(name=\"avg wer\", type=\"mean\", columns=[\"wer\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.upload_dataset(df, id_column=\"id\", data_column=\"data\", label_column=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference\n",
    "\n",
    "We now run inference on the base Whisper models and Distil models, cacheing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"medium.en\", \"large-v1\", \"large-v2\", \"large-v3\", \"distil-medium.en\", \"distil-large-v2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "\n",
    "df_systems = []\n",
    "for model_name in models:\n",
    "    try:\n",
    "        df_system = pd.read_parquet(f\"cache/{model_name}.parquet\")\n",
    "    except:\n",
    "        df_system = df[[\"id\", \"data\", \"label\"]].copy()\n",
    "\n",
    "        if \"distil\" in model_name:\n",
    "            model_id = \"distil-whisper/\" + model_name\n",
    "            model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "                model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "            )\n",
    "            model.to(device)\n",
    "\n",
    "            processor = AutoProcessor.from_pretrained(model_id)\n",
    "            pipe = pipeline(\n",
    "                \"automatic-speech-recognition\",\n",
    "                model=model,\n",
    "                tokenizer=processor.tokenizer,\n",
    "                feature_extractor=processor.feature_extractor,\n",
    "                max_new_tokens=128,\n",
    "                chunk_length_s=15,\n",
    "                batch_size=16,\n",
    "                torch_dtype=torch_dtype,\n",
    "                device=device,\n",
    "            )\n",
    "            df_system[\"output\"] = df_system[\"data\"].progress_apply(lambda x: pipe(x)['text'])\n",
    "            pass\n",
    "        else:\n",
    "            whisper_model = whisper.load_model(model_name)\n",
    "            df_system[\"output\"] = df_system[\"data\"].progress_apply(lambda x: whisper_model.transcribe(x)[\"text\"])\n",
    "\n",
    "        df_system[\"wer\"] = df_system.progress_apply(lambda x: wer(x[\"label\"], x[\"output\"]), axis=1)\n",
    "        df_system.to_parquet(f\"cache/{model_name}.parquet\", index=False)\n",
    "    df_systems.append(df_system) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Results\n",
    "\n",
    "Lastly, we upload our final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_system in enumerate(df_systems):\n",
    "    project.upload_system(df_system[[\"id\", \"output\", \"wer\"]], name=models[i], id_column=\"id\", output_column=\"output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
