{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Import libraries, set up model, and read input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from zeno_client import ZenoClient, ZenoMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"tax-benchmark.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(input):\n",
    "    return_question = input[\"source_question\"][\"description\"].replace(\"\\\\n\", \"\\n\")\n",
    "    return_question += \"\\n\\n\"\n",
    "    for answer in enumerate(input[\"source_question\"][\"options\"]):\n",
    "        return_question += f\"{answer[0] + 1}. {answer[1]}\\n\"\n",
    "    return return_question\n",
    "\n",
    "\n",
    "df_input = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [format_question(d) for d in data],\n",
    "        \"answer\": [str(d[\"source_question\"][\"correct_answer\"]) for d in data],\n",
    "        \"reference\": [d[\"source_question\"][\"reference\"] for d in data],\n",
    "        \"tag\": [d[\"source_question\"][\"tag\"] for d in data],\n",
    "        \"category\": [d[\"source_question\"][\"category\"] for d in data],\n",
    "    }\n",
    ")\n",
    "df_input[\"question length\"] = df_input[\"question\"].apply(lambda x: len(x))\n",
    "df_input[\"id\"] = df_input.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional, generate topics using BERTopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\"english\", min_topic_size=3)\n",
    "topics, probs = topic_model.fit_transform(\n",
    "    [d[\"source_question\"][\"description\"] for d in data]\n",
    ")\n",
    "df_input[\"topic\"] = topics\n",
    "df_input[\"topic\"] = df_input[\"topic\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Zeno Project\n",
    "\n",
    "Our view configuration will feature markdown for the input data and the system output.\n",
    "We'll add two metrics, accuracy and output length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../.env\", override=True)\n",
    "client = ZenoClient(os.environ.get(\"ZENO_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = client.create_project(\n",
    "    name=\"LLM Taxes Benchmark\",\n",
    "    view={\n",
    "        \"data\": {\"type\": \"markdown\"},\n",
    "        \"label\": {\"type\": \"text\"},\n",
    "        \"output\": {\"type\": \"markdown\"},\n",
    "    },\n",
    "    description=\"Tax questions for LLMs\",\n",
    "    public=True,\n",
    "    metrics=[\n",
    "        ZenoMetric(name=\"accuracy\", type=\"mean\", columns=[\"correct\"]),\n",
    "        ZenoMetric(name=\"output length\", type=\"mean\", columns=[\"output length\"]),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.upload_dataset(\n",
    "    df_input, id_column=\"id\", data_column=\"question\", label_column=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in data[0][\"full\"].keys():\n",
    "    df_system = pd.DataFrame(\n",
    "        {\n",
    "            \"output\": [\n",
    "                f\"**Full:** {d['full'][model]}\\n\\n**Simplified**: {d['simplified'][model]}\"\n",
    "                for d in data\n",
    "            ],\n",
    "            \"output length\": [len(d[\"full\"][model]) for d in data],\n",
    "            \"simplified output\": [str(d[\"simplified\"][model]) for d in data],\n",
    "        }\n",
    "    )\n",
    "    df_system[\"correct\"] = df_input[\"answer\"] == df_system[\"simplified output\"]\n",
    "    df_system[\"id\"] = df_input[\"id\"]\n",
    "    project.upload_system(\n",
    "        df_system, name=model.replace(\"/\", \"-\"), id_column=\"id\", output_column=\"output\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
