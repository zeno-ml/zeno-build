{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeno Build Tutorial 3: Evaluating Text Generation\n",
    "\n",
    "In this tutorial, we'll how to use\n",
    "[Zeno Build](https://github.com/zeno-ml/zeno-build/) to evaluate generated text.\n",
    "We'll assume that you've already read the\n",
    "[previous tutorial](02_inference.ipynb) and have a basic understanding of\n",
    "how to use Zeno Build to visualize results and perform inference.\n",
    "\n",
    "Specifically, we will use models from [Hugging Face](https://huggingface.com) and [OpenAI](https://openai.com) to perform French-English translation on data from [Ted Talks](https://ted.org). To evaluate the generated text, we will use the [Critique API](https://docs.inspiredco.ai/critique/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Make sure that Zeno Build is installed (`pip install zeno-build`) and that you do the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from zeno_build.evaluation.text_features.length import input_length, output_length\n",
    "from zeno_build.evaluation.text_metrics.critique import (\n",
    "    avg_bert_score,\n",
    "    avg_chrf,\n",
    "    bert_score,\n",
    "    chrf,\n",
    ")\n",
    "from zeno_build.experiments.experiment_run import ExperimentRun\n",
    "from zeno_build.models.lm_config import LMConfig\n",
    "from zeno_build.models.text_generate import generate_from_text_prompt\n",
    "from zeno_build.reporting.visualize import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses OpenAI and Critique, so also make sure that you:\n",
    "* Obtain an [OpenAI API key]() and set in the environment variable `OPENAI_API_KEY`\n",
    "* Obtain a [Inspired Cognition API Key](https://dashboard.inspiredco.ai) and set in the environment variable `INSPIREDCO_API_KEY`\n",
    "\n",
    "The best way to set these variables is to create a file called `.env` in the same directory as this notebook, and put the following line in it:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<your key here>\n",
    "INSPIREDCO_API_KEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Data\n",
    "\n",
    "Next we'll process the necessary data. We'll use the [ted_multi](https://huggingface.co/datasets/ted_multi) dataset from Hugging Face. We'll use 250 sentences from the French-English subset of this dataset for efficiency purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ted_multi\", split=\"validation\")\n",
    "srcs, trgs = [], []\n",
    "src_language, trg_language = \"fr\", \"en\"\n",
    "for datum in dataset:\n",
    "    if (\n",
    "        src_language not in datum[\"translations\"][\"language\"]\n",
    "        or trg_language not in datum[\"translations\"][\"language\"]\n",
    "    ):\n",
    "        continue\n",
    "    src_index = datum[\"translations\"][\"language\"].index(src_language)\n",
    "    trg_index = datum[\"translations\"][\"language\"].index(trg_language)\n",
    "    srcs.append(datum[\"translations\"][\"translation\"][src_index])\n",
    "    trgs.append(datum[\"translations\"][\"translation\"][trg_index])\n",
    "    if len(srcs) >= 250:\n",
    "        break\n",
    "df = pd.DataFrame({\"text\": srcs, \"label\": trgs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "\n",
    "Next, we generate outputs. We can define the prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = (\n",
    "    \"Translate this sentence into English:\\n\\n\" \"Sentence: {{text}}\\n\" \"English: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we use this to perform inference with various language models, like we did in the previous tutorial. The only difference is that we'll increase the number of tokens and temperature, which is more conducive to generating longer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results = []\n",
    "for lm_config in [\n",
    "    LMConfig(provider=\"openai_chat\", model=\"gpt-3.5-turbo\"),\n",
    "    LMConfig(provider=\"huggingface\", model=\"gpt2\"),\n",
    "    LMConfig(provider=\"huggingface\", model=\"gpt2-xl\"),\n",
    "]:\n",
    "    predictions = generate_from_text_prompt(\n",
    "        [{\"text\": x} for x in srcs],\n",
    "        prompt_template=prompt_template,\n",
    "        model_config=lm_config,\n",
    "        temperature=0.3,\n",
    "        max_tokens=200,\n",
    "        top_p=1.0,\n",
    "        requests_per_minute=400,\n",
    "    )\n",
    "    result = ExperimentRun(\n",
    "        name=lm_config.model,\n",
    "        parameters={\"provider\": lm_config.provider, \"model\": lm_config.model},\n",
    "        predictions=[x.strip().split(\"\\n\")[0] for x in predictions],\n",
    "    )\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Outputs and Visualizing\n",
    "\n",
    "Finally, we'll perform evaluation and visualization. Instead of using exact match (accuracy) to evaluate the outputs like we did in the previous tutorial, we'll use [chrf](https://aclanthology.org/W15-3049/) (a measure of string similarity between the gold-standard output and the generated output) and [BERTScore](https://arxiv.org/abs/1904.09675) (a measure of semantic similarity between the two outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    output_length,\n",
    "    input_length,\n",
    "    chrf,\n",
    "    avg_chrf,\n",
    "    bert_score,\n",
    "    avg_bert_score,\n",
    "]\n",
    "\n",
    "visualize(\n",
    "    df,\n",
    "    trgs,\n",
    "    all_results,\n",
    "    \"text-classification\",\n",
    "    \"text\",\n",
    "    functions,\n",
    "    zeno_config={\"cache_path\": \"zeno_cache\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This is the end of the tutorial series for now!\n",
    "Next you can click over to the [examples](../../examples/) directory to see some more end-to-end examples of how to use Zeno Build to run experiments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
