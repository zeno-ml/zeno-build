{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeno Build Tutorial 2: Performing Inference\n",
    "\n",
    "In this tutorial, we'll how to use\n",
    "[Zeno Build](https://github.com/zeno-ml/zeno-build/) to perform inference with a\n",
    "variety of LLMs and visualize/compare the results.\n",
    "We'll assume that you've already read the\n",
    "[previous tutorial](01_visualization.ipynb) and have a basic understanding of\n",
    "how to use Zeno Build to visualize results.\n",
    "\n",
    "Specifically, we will use models from [Hugging Face](https://huggingface.com) and [OpenAI](https://openai.com) to predict the sentiment of English movie reviews from the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we do some imports. Also make sure that your OpenAI key is set in your `.env` file like in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from zeno_build.evaluation.text_features.exact_match import avg_exact_match, exact_match\n",
    "from zeno_build.evaluation.text_features.length import input_length, output_length\n",
    "from zeno_build.experiments.experiment_run import ExperimentRun\n",
    "from zeno_build.models.lm_config import LMConfig\n",
    "from zeno_build.models.text_generate import generate_from_text_prompt\n",
    "from zeno_build.reporting.visualize import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "Next, we'll import the Stanford Sentiment Treebank from Hugging Face. After performing this import, we will have the input text in `text` and output label in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "data = list(dataset[\"sentence\"])\n",
    "label_map = dataset.features[\"label\"].names\n",
    "labels = [label_map[label] for label in dataset[\"label\"]]\n",
    "df = pd.DataFrame({\"text\": data, \"label\": labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a few templates that we use to prompt the model. We'll use different ones for the models we use on Hugging Face (because they mostly specialize in completing text), and for models using OpenAI (because they are more chat-based models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = {\n",
    "    \"huggingface\": (\n",
    "        \"Review: {{text}}\\n\\n\"\n",
    "        \"Q: Is this review a negative or positive review?\\n\\nA: It is a\"\n",
    "    ),\n",
    "    \"openai_chat\": (\n",
    "        \"Review: {{text}}\\n\\n\"\n",
    "        \"Please answer with one word. \"\n",
    "        \"Is this review a negative or positive review?\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform inference. We loop over three models, defined using Zeno Build's `LMConfig` class.\n",
    "This class defines a \"provider\" (e.g. Hugging Face or OpenAI) and a model name.\n",
    "\n",
    "Next we call the `generate_from_text_prompt()` function. This function takes several arguments:\n",
    "* *Inputs:* Which take the form of a list of dictionaries, where each dictionary contains one or more keys corresponding to the places in the prompt to be filled in.\n",
    "* *Prompt Template:* A template like the ones we defined above. It can have one or more slots (like `{{text}}`) that are filled in from elements of the input dictionaries.\n",
    "* *Model Config:* The model configuration as an `LMConfig` object.\n",
    "* *Generation Parameters:* Including things like `temperature`, `max_tokens`, and `top_p`, mirroring the OpenAI API.\n",
    "* *Requests Per Minute:* For API-based models such as OpenAI, the maximum number of requests to send per minute (to avoid rate limiting).\n",
    "\n",
    "Based on the results of this we create an `ExperimentRun`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for lm_config in [\n",
    "    LMConfig(provider=\"openai_chat\", model=\"gpt-3.5-turbo\"),\n",
    "    LMConfig(provider=\"huggingface\", model=\"gpt2\"),\n",
    "    LMConfig(provider=\"huggingface\", model=\"gpt2-xl\"),\n",
    "]:\n",
    "    predictions = generate_from_text_prompt(\n",
    "        [{\"text\": x} for x in data],\n",
    "        prompt_template=prompt_templates[lm_config.provider],\n",
    "        model_config=lm_config,\n",
    "        temperature=0.0001,\n",
    "        max_tokens=1,\n",
    "        top_p=1.0,\n",
    "        requests_per_minute=400,\n",
    "    )\n",
    "    result = ExperimentRun(\n",
    "        name=lm_config.model,\n",
    "        parameters={\"provider\": lm_config.provider, \"model\": lm_config.model},\n",
    "        predictions=[x.strip().lower() for x in predictions],\n",
    "    )\n",
    "    all_results.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the outputs should take several minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Evaluation/Visualization/Analysis\n",
    "\n",
    "Next we define functions for analysis, and then call the `visualize` function to perform visualization, as we did in [the previous tutorial](01_visualization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "functions = [\n",
    "    output_length,\n",
    "    input_length,\n",
    "    exact_match,\n",
    "    avg_exact_match,\n",
    "]\n",
    "\n",
    "visualize(\n",
    "    df,\n",
    "    labels,\n",
    "    all_results,\n",
    "    \"text-classification\",\n",
    "    \"text\",\n",
    "    functions,\n",
    "    zeno_config={\"cache_path\": \"zeno_cache\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Up until now, we've been focusing on text classification. In the [next tutorial](03_text_generation.ipynb), we'll look at how to use Zeno Build to evaluate text generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeno-build",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
